{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-Project 2 \n",
    "## Multi-Class Logistic Regression and Gradient Descent\n",
    "\n",
    "Jiahua Liang   260711529    \n",
    "Irene Huang    260740689 \n",
    "Tracy Zhou  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent: \n",
    "    # TODO: add validation set here \n",
    "    def __init__(self, lr, max_iter, epsilon, beta, batch_size, recording_history=False):\n",
    "        self.lr = lr\n",
    "        # \n",
    "        \n",
    "        \n",
    "    def run(self, gradient_fn, mini_batches, w):\n",
    "        # Step 1: Split into mini-batch \n",
    "            # Shuffle data \n",
    "            # Split into mini-batch and their corresponding y\n",
    "            # mini_batches = [mini_batch1, mini_batch2, ....]\n",
    "            # mini_ys = [mini_y1, mini_y2,....]\n",
    "    \n",
    "        while # < max_iter and ....:\n",
    "            for mini_batch in mini_batches: \n",
    "                #TODO: find mini_y in mini_ys\n",
    "                \n",
    "                # Step 2: Calculate Gradient\n",
    "                grad = gradient_fn(mini_batch, mini_y, w)  # softmax_cost_fn\n",
    "\n",
    "                # Step 3: Update W using momentum \n",
    "                self.w = ...\n",
    "                \n",
    "                # Step 4: Calculate training & validation set accuracy using self.w\n",
    "                    # TODO: Termination Condition \n",
    "                \n",
    "                \n",
    "        \n",
    "        return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, w):\n",
    "    z = np.dot(x,w)\n",
    "    z -= np.max(z)\n",
    "    prob = np.exp(z) / np.sum(np.exp(z), axis=0)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_softmax_fn(x, y, w):\n",
    "    yh = softmax(x, w)\n",
    "    grad = np.dot((yh - y), x)\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class Logistic Regression \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic_Regression:\n",
    "    def __init__(self, add_bias=True):\n",
    "        self.add_bias = add_bias \n",
    "    \n",
    "    def fit(self, x, y, optimizer):\n",
    "        if x.ndim == 1:\n",
    "            x = x[:, None]\n",
    "        if self.add_bias:\n",
    "            N = x.shape[0]\n",
    "            x = np.column_stack([x, np.ones[N]])\n",
    "                  \n",
    "        w0 = np.zeros(D)\n",
    "        self.w = optimzer.run(cost_softmax_fn, x, y, w0)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "        if x.ndim == 1:\n",
    "            x = x[:, None]\n",
    "        Np = x.shape[0]\n",
    "        if self.add_bias: \n",
    "            x = np.column_stack([x, np.ones(Np)])\n",
    "            \n",
    "        # Softmax \n",
    "        yh = softmax(x, self.w)\n",
    "        \n",
    "        return yh \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(y_predict, y): \n",
    "    boolean = [np.argmax(yh[i]) == np.argmax(y[i]) for i in range(y.shape[0])]\n",
    "    correct_count = 0\n",
    "    for i in boolean:\n",
    "        if i == True:\n",
    "            correct_count+=1\n",
    "    total_count = y.shape[0]\n",
    "    accuracy = correct_count/total_count\n",
    "\n",
    "    return accuracy \n",
    "\n",
    "\n",
    "# Hyperparameters \n",
    "batch_size = 16\n",
    "learning_rate = 0.01 \n",
    "max_iter = 1000\n",
    "epsilon = 0.08\n",
    "# TODO: Grid Search -> find the best one  \n",
    "# TODO: Run time \n",
    "# Need a for loop \n",
    "\n",
    "optimizer = GradientDescent(learning_rate, max_iter, epsilon, beta, batch_size, True)\n",
    "model = Logistic_Regression()\n",
    "# ! CHECK training & label name \n",
    "model.fit(x, y, optimzer)\n",
    "\n",
    "# TODO: Plot hyperparameters vs accuracy \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
