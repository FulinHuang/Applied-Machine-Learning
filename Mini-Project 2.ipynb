{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-Project 2 \n",
    "## Multi-Class Logistic Regression and Gradient Descent\n",
    "\n",
    "Jiahua Liang   260711529    \n",
    "Irene Huang    260740689 \\\n",
    "Tracy Zhou  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  5. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ... 10.  0.  0.]\n",
      " [ 0.  0.  0. ... 16.  9.  0.]\n",
      " ...\n",
      " [ 0.  0.  1. ...  6.  0.  0.]\n",
      " [ 0.  0.  2. ... 12.  0.  0.]\n",
      " [ 0.  0. 10. ... 12.  1.  0.]]\n",
      "(1797, 64)\n",
      "[0 1 2 ... 8 9 8]\n",
      "1797\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits \n",
    "digits = load_digits() \n",
    "print(digits.data)\n",
    "print(digits.data.shape)\n",
    "print(digits.target)\n",
    "print(len(digits.target))\n",
    "# Check nan \n",
    "print(np.isnan(digits.data).any())\n",
    "# One-hot encoding \n",
    "# TODO: K-Fold cross validation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 2: Vehicle https://www.openml.org/d/54\n",
    "\n",
    "# upload csv\n",
    "\n",
    "# Convert to pandas \n",
    "df = dataFrame(... )\n",
    "\n",
    "# Convert from pandas to numpy \n",
    "array = df.values \n",
    "\n",
    "# one-hot encoding for labels \n",
    "data = array[:, 0:-2]\n",
    "target = one_hot_encoding(array[:,-1])\n",
    "\n",
    "# Check nan \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Function \n",
    "Define the batch size and split the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent: \n",
    "    # TODO: add validation set here \n",
    "    def __init__(self, lr, max_iter, epsilon, beta, batch_size, recording_history=False):\n",
    "        self.lr = lr\n",
    "        # \n",
    "        \n",
    "        \n",
    "    def run(self, gradient_fn, mini_batches, w):\n",
    "        # Step 1: Split into mini-batch \n",
    "            # Shuffle data \n",
    "            # Split into mini-batch and their corresponding y\n",
    "            # mini_batches = [mini_batch1, mini_batch2, ....]\n",
    "            # mini_ys = [mini_y1, mini_y2,....]\n",
    "    \n",
    "        while # < max_iter and ....:\n",
    "            for mini_batch in mini_batches: \n",
    "                #TODO: find mini_y in mini_ys\n",
    "                \n",
    "                # Step 2: Calculate Gradient\n",
    "                grad = gradient_fn(mini_batch, mini_y, w)  # softmax_cost_fn\n",
    "\n",
    "                # Step 3: Update W using momentum \n",
    "                self.w = ...\n",
    "                \n",
    "                # Step 4: Calculate training & validation set accuracy using self.w\n",
    "                    # TODO: Termination Condition \n",
    "                \n",
    "                \n",
    "        \n",
    "        return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, w):\n",
    "    z = np.dot(x,w)\n",
    "    z -= np.max(z)\n",
    "    prob = (np.exp(z).T / np.sum(np.exp(z), axis=1)).T\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_softmax_fn(x, y, w):\n",
    "    \n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class Logistic Regression \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic_Regression:\n",
    "    def __init__(self, add_bias=True):\n",
    "        self.add_bias = add_bias \n",
    "    \n",
    "    def fit(self, x, y, optimizer):\n",
    "        if x.ndim == 1:\n",
    "            x = x[:, None]\n",
    "        if self.add_bias:\n",
    "            N = x.shape[0]\n",
    "            x = np.column_stack([x, np.ones[N]])\n",
    "                  \n",
    "        w0 = np.zeros(D)\n",
    "        self.w = optimzer.run(cost_softmax_fn, x, y, w0)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "        if x.ndim == 1:\n",
    "            x = x[:, None]\n",
    "        Np = x.shape[0]\n",
    "        if self.add_bias: \n",
    "            x = np.column_stack([x, np.ones(Np)])\n",
    "            \n",
    "        # Softmax \n",
    "        yh = softmax(x, self.w)\n",
    "        # TODO: argmax...  result: yh = [0 0 1 0 0], [0 1 0 0 0]\n",
    "        \n",
    "        return yh \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(): \n",
    "    return accuracy \n",
    "\n",
    "\n",
    "# Hyperparameters \n",
    "batch_size = 16\n",
    "learning_rate = 0.01 \n",
    "max_iter = 1000\n",
    "epsilon = 0.08\n",
    "# TODO: Grid Search -> find the best one  \n",
    "# TODO: Run time \n",
    "# Need a for loop \n",
    "\n",
    "optimizer = GradientDescent(learning_rate, max_iter, epsilon, beta, batch_size, True)\n",
    "model = Logistic_Regression()\n",
    "# ! CHECK training & label name \n",
    "model.fit(x, y, optimzer)\n",
    "\n",
    "# TODO: Plot hyperparameters vs accuracy \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
